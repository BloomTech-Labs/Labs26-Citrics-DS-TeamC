{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of combined_v1",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lambda-School-Labs/Labs26-Citrics-DS-TeamC/blob/mtoce/notebooks/Copy_of_combined_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Oxjpnh0nAyk"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgnz6795ANJM",
        "outputId": "d48ea3e4-8725-4b14-c740-a05389506124",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# MOUNT Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWg5DqkS9y1_"
      },
      "source": [
        "# PATH and load info\n",
        "base_path = '/content/drive/My Drive/lambda_school/driftly_data/'\n",
        "\n",
        "weather = pd.read_csv(base_path + 'weather_data/weather_insert_final.csv')\n",
        "housing = pd.read_csv(base_path + 'housing_data/housing_merge.csv')\n",
        "jobs = pd.read_csv(base_path + 'jobs_data/jobs_merge.csv')\n",
        "covid = pd.read_csv(base_path + 'covid_data/covid_merge.csv')\n",
        "pop = pd.read_csv(base_path + 'population_data/city_pop.csv')\n",
        "keys = pd.read_csv(base_path + 'database_tables/keys_pop.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGE_orOenNly"
      },
      "source": [
        "# Geography"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WibPGqinnOpi"
      },
      "source": [
        "geographic_breakdown = pd.read_csv(base_path + 'population_data/geographic_breakdown.csv')\n",
        "geographic_breakdown.drop(columns=['Unnamed: 0'], inplace=True)\n",
        "\n",
        "## CHECK:\n",
        "print(geographic_breakdown.shape)\n",
        "geographic_breakdown.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZOaz7TR7tIL"
      },
      "source": [
        "import psycopg2\n",
        "from sqlalchemy import create_engine\n",
        "from sqlalchemy.types import Integer, Float, Text, String, DateTime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGGHCfcbGwfh"
      },
      "source": [
        "db_flavor = \"postgres\"\n",
        "db_python_library = \"psycopg2\"\n",
        "db_username = \"postgres\"\n",
        "db_password = \"UM*3tAe<ns!q!nG3\"\n",
        "db_host = \"citrics-ds-teamc.cav8gkdxva9e.us-east-1.rds.amazonaws.com\"\n",
        "db_port = \"5432\"\n",
        "db_name = \"postgres\"\n",
        "db_uri = db_flavor + \"+\" + db_python_library + \"://\" + db_username + \":\" + db_password + \"@\" + db_host + \":\" + db_port + \"/\" + db_name\n",
        "\n",
        "# CREATE engine\n",
        "engine = create_engine(db_uri, echo=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhqqKo4C76lG"
      },
      "source": [
        "geographic_breakdown.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bumGQBZWGwdM"
      },
      "source": [
        "# table_name = 'geographic_breakdown'\n",
        "\n",
        "# geographic_breakdown.to_sql(table_name, engine, if_exists='replace', index=False, chunksize=500,\n",
        "#             dtype={\n",
        "#                 \"region_id\": Integer,\n",
        "#                 \"postal\": String,\n",
        "#                 \"metro\": String,\n",
        "#                 \"county\": String,\n",
        "#                 \"city\": String,\n",
        "#                 \"region_type\": String,\n",
        "#                 \"city-state\": String,\n",
        "#                 \"county-state\": String,\n",
        "#                 \"region_info\": String\n",
        "#             })"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSZKTF1cGwa4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkCyaQChGwX4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zThCHqntnS-R"
      },
      "source": [
        "geo_dictionary = {}\n",
        "\n",
        "for postal in geographic_breakdown['postal']:\n",
        "    geo_dictionary[postal] = {}\n",
        "\n",
        "for idx, val in enumerate(geographic_breakdown['county_state']):\n",
        "    postal = val[-2:]\n",
        "    if postal in geo_dictionary:\n",
        "        geo_dictionary[postal][val] = set()\n",
        "\n",
        "for idx, val in enumerate(geographic_breakdown['city_state']):\n",
        "    postal = val[-2:]\n",
        "    county_state = geographic_breakdown['county_state'][idx]\n",
        "    \n",
        "    if postal in geo_dictionary:\n",
        "        if county_state in geo_dictionary[postal]:\n",
        "            geo_dictionary[postal][county_state].add(val) \n",
        "\n",
        "\n",
        "# comprehensive_dictionary\n",
        "geo_dictionary['NY']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "noP3daLDnTAx"
      },
      "source": [
        "geo_regionid_lst = geographic_breakdown['region_id'].to_list()\n",
        "# len(geo_regionid_lst)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXFH49V5nWZo"
      },
      "source": [
        "# Real Estate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzbQEjQ9nTDO",
        "outputId": "b27b1a11-099a-4036-f5da-ce33a6534192",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "ZATT = pd.read_csv(base_path + 'housing_data/ZATT_data_full.csv')\n",
        "ZATT.drop(columns=['Unnamed: 0'], inplace=True)\n",
        "\n",
        "## CHECK:\n",
        "print(ZATT.shape)\n",
        "ZATT.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (10) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(6010825, 14)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>indicator_id</th>\n",
              "      <th>region_id</th>\n",
              "      <th>region_type</th>\n",
              "      <th>date</th>\n",
              "      <th>year</th>\n",
              "      <th>month</th>\n",
              "      <th>value</th>\n",
              "      <th>city</th>\n",
              "      <th>county</th>\n",
              "      <th>metro</th>\n",
              "      <th>postal</th>\n",
              "      <th>region_info</th>\n",
              "      <th>city_state</th>\n",
              "      <th>county_state</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ZATT</td>\n",
              "      <td>6181</td>\n",
              "      <td>city</td>\n",
              "      <td>2020-07-31</td>\n",
              "      <td>2020</td>\n",
              "      <td>2020-07</td>\n",
              "      <td>1143222.0</td>\n",
              "      <td>New York</td>\n",
              "      <td>Queens County</td>\n",
              "      <td>New York-Newark-Jersey City</td>\n",
              "      <td>NY</td>\n",
              "      <td>New York; NY; New York-Newark-Jersey City; Que...</td>\n",
              "      <td>New York, NY</td>\n",
              "      <td>Queens County, NY</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ZATT</td>\n",
              "      <td>6181</td>\n",
              "      <td>city</td>\n",
              "      <td>2020-06-30</td>\n",
              "      <td>2020</td>\n",
              "      <td>2020-06</td>\n",
              "      <td>1143665.0</td>\n",
              "      <td>New York</td>\n",
              "      <td>Queens County</td>\n",
              "      <td>New York-Newark-Jersey City</td>\n",
              "      <td>NY</td>\n",
              "      <td>New York; NY; New York-Newark-Jersey City; Que...</td>\n",
              "      <td>New York, NY</td>\n",
              "      <td>Queens County, NY</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ZATT</td>\n",
              "      <td>6181</td>\n",
              "      <td>city</td>\n",
              "      <td>2020-05-31</td>\n",
              "      <td>2020</td>\n",
              "      <td>2020-05</td>\n",
              "      <td>1143204.0</td>\n",
              "      <td>New York</td>\n",
              "      <td>Queens County</td>\n",
              "      <td>New York-Newark-Jersey City</td>\n",
              "      <td>NY</td>\n",
              "      <td>New York; NY; New York-Newark-Jersey City; Que...</td>\n",
              "      <td>New York, NY</td>\n",
              "      <td>Queens County, NY</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ZATT</td>\n",
              "      <td>12447</td>\n",
              "      <td>city</td>\n",
              "      <td>2020-07-31</td>\n",
              "      <td>2020</td>\n",
              "      <td>2020-07</td>\n",
              "      <td>1492845.0</td>\n",
              "      <td>Los Angeles</td>\n",
              "      <td>Los Angeles County</td>\n",
              "      <td>Los Angeles-Long Beach-Anaheim</td>\n",
              "      <td>CA</td>\n",
              "      <td>Los Angeles; CA; Los Angeles-Long Beach-Anahei...</td>\n",
              "      <td>Los Angeles, CA</td>\n",
              "      <td>Los Angeles County, CA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ZATT</td>\n",
              "      <td>12447</td>\n",
              "      <td>city</td>\n",
              "      <td>2020-06-30</td>\n",
              "      <td>2020</td>\n",
              "      <td>2020-06</td>\n",
              "      <td>1476789.0</td>\n",
              "      <td>Los Angeles</td>\n",
              "      <td>Los Angeles County</td>\n",
              "      <td>Los Angeles-Long Beach-Anaheim</td>\n",
              "      <td>CA</td>\n",
              "      <td>Los Angeles; CA; Los Angeles-Long Beach-Anahei...</td>\n",
              "      <td>Los Angeles, CA</td>\n",
              "      <td>Los Angeles County, CA</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  indicator_id  region_id  ...       city_state            county_state\n",
              "0         ZATT       6181  ...     New York, NY       Queens County, NY\n",
              "1         ZATT       6181  ...     New York, NY       Queens County, NY\n",
              "2         ZATT       6181  ...     New York, NY       Queens County, NY\n",
              "3         ZATT      12447  ...  Los Angeles, CA  Los Angeles County, CA\n",
              "4         ZATT      12447  ...  Los Angeles, CA  Los Angeles County, CA\n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0ZQNtHSnTFZ"
      },
      "source": [
        "def RE_json_func(ZATT, year_month_day, geo_regionid_lst):\n",
        "    \n",
        "    ## subsetting df for columns\n",
        "    ZATT_subset = ZATT.copy()\n",
        "    ZATT_subset = ZATT_subset[ZATT_subset['region_id'].isin(geo_regionid_lst)]\n",
        "    ZATT_subset = ZATT_subset[['date', 'indicator_id', 'region_id', 'city', 'postal', 'city_state', 'value']]\n",
        "\n",
        "    ## subsetting df for date\n",
        "    df_date = ZATT_subset[ZATT_subset['date'] >= year_month_day]\n",
        "    df_date.reset_index(inplace=True)\n",
        "\n",
        "    ## creating storage dictionary\n",
        "    state_cities_dictionary = {}\n",
        "            \n",
        "    idx = 0\n",
        "    for i in df_date['postal']:\n",
        "        if i not in state_cities_dictionary:\n",
        "            state_cities_dictionary[i] = []\n",
        "        else:\n",
        "            zip_ = [df_date['city_state'][idx], df_date['region_id'][idx], df_date['value'][idx]]\n",
        "            state_cities_dictionary[i].append(zip_)\n",
        "        idx += 1\n",
        "\n",
        "\n",
        "    ## inserting values into storage dictionary\n",
        "    for i in state_cities_dictionary:\n",
        "        city_state_combo = [lst[0] for lst in state_cities_dictionary[i]]   \n",
        "        value = [lst[2] for lst in state_cities_dictionary[i]]\n",
        "        \n",
        "        combo_value_dict = dict(zip(city_state_combo, value))\n",
        "\n",
        "        state_cities_dictionary[i] = combo_value_dict\n",
        "\n",
        "    ## converting to JSON\n",
        "    final_states_city_dictionary = json.dumps(state_cities_dictionary)\n",
        "    \n",
        "    ## return: final_states_city_dictionary\n",
        "    return final_states_city_dictionary\n",
        "\n",
        "\n",
        "## CHECK:\n",
        "RE_JSON = RE_json_func(ZATT, '2020-07-31', geo_regionid_lst)\n",
        "# RE_JSON"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xu2l48NwnTHu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxBTTWaHncV5"
      },
      "source": [
        "# Jobs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWPwM0J2nTKK",
        "outputId": "2c064ce4-f662-4829-ebbb-1531c64eb719",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "# Wage / Earnings Data\n",
        "wage_data = pd.read_csv(r'.\\combo_data\\jobs\\quarterly_data_copy.csv')\n",
        "\n",
        "# CHECK:\n",
        "print(wage_data.shape)\n",
        "wage_data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-99df68f4ccbc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Wage / Earnings Data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mwage_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'.\\combo_data\\jobs\\quarterly_data_copy.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# CHECK:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwage_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    684\u001b[0m     )\n\u001b[1;32m    685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    934\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 936\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    937\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1166\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1168\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1169\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1170\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1996\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1997\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1998\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1999\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '.\\\\combo_data\\\\jobs\\\\quarterly_data_copy.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_L9Z9YqnTMS"
      },
      "source": [
        "def wage_func(df, year, quarter):\n",
        "    \n",
        "    ## Copying and cleaning df\n",
        "    X = df.copy()\n",
        "    \n",
        "    X = X[(X['Year'] == year) &\n",
        "          (X['Qtr'] == quarter)]\n",
        "\n",
        "    X.reset_index(inplace=True)\n",
        "    X.drop(columns=['index', 'Unnamed: 0'], inplace=True)\n",
        "\n",
        "\n",
        "    ## POSTAL DICT:\n",
        "    postal_dict = {}\n",
        "\n",
        "    idx = 0\n",
        "    for i in X['postal']:\n",
        "        postal_dict[i] = {}\n",
        "\n",
        "    for i in X['county_state']:\n",
        "        if i[-2:] in postal_dict:\n",
        "            postal_dict[i[-2:]][i] = {}\n",
        "\n",
        "\n",
        "    ## COUNTY DICT:\n",
        "    county_state_dict = {}\n",
        "    for i in X['county_state']:\n",
        "        county_state_dict[i] = {}\n",
        "\n",
        "    for idx, ind in enumerate(X['Industry']):\n",
        "        county_state = X['county_state'][idx]\n",
        "        if county_state in county_state_dict:\n",
        "            county_state_dict[county_state][ind] = {}\n",
        "\n",
        "\n",
        "    # GROUPBY\n",
        "    df_groupby = X.groupby(['county_state','Industry']).mean()\n",
        "\n",
        "    # creating groupby dictionary\n",
        "    groupby_dict = df_groupby.to_dict('index')\n",
        "\n",
        "    # looping through the dictionary and substituting dictionary values\n",
        "    for county_state in county_state_dict:\n",
        "        for industry in county_state_dict[county_state]:\n",
        "                county_state_dict[county_state][industry] = groupby_dict[county_state, industry]\n",
        "\n",
        "    # looping through the postal_dict and replacing the value item with the supplemental county_state dict value item\n",
        "    for i in postal_dict:\n",
        "        countstate_dict = postal_dict[i]\n",
        "        for j in countstate_dict:\n",
        "            if j in county_state_dict:\n",
        "                countstate_dict[j] = county_state_dict[j]\n",
        "\n",
        "    # reutrn\n",
        "    return postal_dict\n",
        "\n",
        "\n",
        "# CHCEK:\n",
        "wage_dict = wage_func(wage_data, 2019, 3)\n",
        "# wage_dict['NY']['New York, NY']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKYRUT9rnTOk"
      },
      "source": [
        "# Business Dynmaics Data\n",
        "biz_dyn_national = pd.read_csv(r'.\\combo_data\\jobs\\jobs_business_dynamics_copy.csv')\n",
        "biz_dyn_national.drop(columns='Unnamed: 0', inplace=True)\n",
        "\n",
        "# CHECK:\n",
        "print(biz_dyn_national.shape)\n",
        "biz_dyn_national.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxTO8exjnTQz"
      },
      "source": [
        "# National JOLTS Data\n",
        "jolts_national = pd.read_csv(r'.\\combo_data\\jobs\\jobs_jolts_national_copy.csv')\n",
        "jolts_national.drop(columns='Unnamed: 0', inplace=True)\n",
        "\n",
        "# CHECK:\n",
        "print(jolts_national.shape)\n",
        "jolts_national.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "at9F2e-2nTS2"
      },
      "source": [
        "# Industry JOLTS Data\n",
        "jolts_industry = pd.read_csv(r'.\\combo_data\\jobs\\jobs_jolts_industry_copy.csv')\n",
        "jolts_industry.drop(columns='Unnamed: 0', inplace=True)\n",
        "\n",
        "# CHECK:\n",
        "print(jolts_industry.shape)\n",
        "jolts_industry.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0ErNrXBnium"
      },
      "source": [
        "# Local Area and Unemployment Data\n",
        "lau_state_df = pd.read_csv(r'.\\combo_data\\jobs\\jobs_lau_state_copy.csv')\n",
        "lau_state_df.drop(columns='Unnamed: 0', inplace=True)\n",
        "\n",
        "# CHECK:\n",
        "print(lau_state_df.shape)\n",
        "lau_state_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPINJNQcniw6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKJ5gNGdnnKs"
      },
      "source": [
        "# Weather"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2iAjp6Onize"
      },
      "source": [
        "# weather_data = pd.read_csv(r'.\\combo_data\\weather_data_final.csv')\n",
        "weather_data = weather.copy()\n",
        "## CHECK:\n",
        "# print(weather_data.shape)\n",
        "# weather_data.head()\n",
        "weather_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHUVSX0pni1-"
      },
      "source": [
        "# Creating a single function for ALL the weather data\n",
        "def weather_json_func(df):\n",
        "    \n",
        "    ## PRELIMINARY\n",
        "    def convert_location_to_state_plus_postal(data):\n",
        "        # us state abbr list for capitals with \"-\" in between cities\n",
        "        us_state_abbrev = {'Alabama': 'AL','Alaska': 'AK','American Samoa': 'AS','Arizona': 'AZ','Arkansas': 'AR','California': 'CA','Colorado': 'CO','Connecticut': 'CT','Delaware': 'DE','District-of-Columbia': 'DC','Florida': 'FL','Georgia': 'GA','Guam': 'GU','Hawaii': 'HI','Idaho': 'ID','Illinois': 'IL','Indiana': 'IN','Iowa': 'IA','Kansas': 'KS','Kentucky': 'KY','Louisiana': 'LA','Maine': 'ME','Maryland': 'MD','Massachusetts': 'MA','Michigan': 'MI','Minnesota': 'MN','Mississippi': 'MS','Missouri': 'MO','Montana': 'MT','Nebraska': 'NE','Nevada': 'NV','New-Hampshire': 'NH','New-Jersey': 'NJ','New-Mexico': 'NM','New-York': 'NY','North-Carolina': 'NC','North-Dakota': 'ND','Northern-Mariana-Islands':'MP','Ohio': 'OH','Oklahoma': 'OK','Oregon': 'OR','Pennsylvania': 'PA','Puerto-Rico': 'PR','Rhode-Island': 'RI','South-Carolina': 'SC','South-Dakota': 'SD','Tennessee': 'TN','Texas': 'TX','Utah': 'UT','Vermont': 'VT','Virgin-Islands': 'VI','Virginia': 'VA','Washington': 'WA','West-Virginia': 'WV','Wisconsin': 'WI','Wyoming': 'WY'}\n",
        "    \n",
        "        # split location between city and state and save to \"new\" df\n",
        "        new = data[\"location\"].str.split(\"_\", n = 1, expand = True)\n",
        "\n",
        "        # create \"city\" and \"state\" cols in data df based on \"new\" df\n",
        "        data[\"city\"] = new[0] \n",
        "        data[\"state\"] = new[1]\n",
        "\n",
        "        # map postals over state column and save to new postals column\n",
        "        data[\"postal\"] = data[\"state\"].map(us_state_abbrev)\n",
        "\n",
        "        # concat city + _ +  postal \n",
        "        data['location'] = data['city'] + \"_\" + data['postal']\n",
        "\n",
        "        # drop unnecessary cols\n",
        "        data = data.drop(['city', 'state', 'postal'], axis = 1)\n",
        "\n",
        "        return data\n",
        "\n",
        "    # calling function\n",
        "    # weather_data = convert_location_to_state_plus_postal(df)\n",
        "    \n",
        "    \n",
        "    ## PART 1:\n",
        "    # clean function\n",
        "    def clean(DataFrame):\n",
        "        # copy frame\n",
        "        X = DataFrame.copy()\n",
        "\n",
        "        # drop unneeded cols\n",
        "        X = X.drop(['Unnamed: 0','winddirDegree','moonrise','moonset','moon_illumination','sunrise','sunset','sunHour','tempC'], axis=1)\n",
        "\n",
        "        # clean out this data, since there are issues with it\n",
        "        X = X[X['mintempC'] != 0]\n",
        "        X = X[X['maxtempC'] != 0]\n",
        "\n",
        "        # create temp df to str split location into multiple cols then concat new DF\n",
        "        loc_temp = X['location'].str.split('_', expand=True)\n",
        "        loc_temp = loc_temp.rename(columns={0: \"city\", 1: \"state_abbr\"})\n",
        "        X = pd.concat([X, loc_temp], axis=1)\n",
        "        X = X.drop('location', axis=1)\n",
        "\n",
        "        # rename cols from str.split\n",
        "        X = X.rename(columns={0: \"city\", 1: \"state_abbr\"})\n",
        "\n",
        "        # convert new columns to strings\n",
        "        X['city'] = X['city'].astype('string')\n",
        "        X['state_abbr'] = X['state_abbr'].astype('string')\n",
        "\n",
        "        # replace dashes with spaces in city col\n",
        "        X['city'] = X['city'].str.replace('-', ' ')\n",
        "\n",
        "        # split date_time into categories for splitting df seasonally\n",
        "        X['date_time'] = pd.to_datetime(X['date_time'], infer_datetime_format=True)\n",
        "        X['year'] = X['date_time'].dt.year\n",
        "        X['month'] = X['date_time'].dt.month\n",
        "        X['day'] = X['date_time'].dt.day\n",
        "\n",
        "        # create cols for fahrenheit from temp_c cols\n",
        "        X['maxtempF'] = X['maxtempC'] * (9/5) + 32\n",
        "        X['mintempF'] = X['mintempC'] * (9/5) + 32\n",
        "        X['FeelsLikeF'] = X['FeelsLikeC'] * (9/5) + 32\n",
        "\n",
        "        # adding new column \n",
        "        X['city_state'] = X['city'] + \", \" + X['state_abbr']\n",
        "\n",
        "        # ordering the columns in the desired way\n",
        "        cols = ['city', 'state_abbr', 'city_state', 'maxtempF', 'FeelsLikeF', \n",
        "                'mintempF', 'maxtempC', 'FeelsLikeC', 'mintempC', 'precipMM', \n",
        "                'totalSnow_cm', 'uvIndex', 'DewPointC', 'HeatIndexC', 'WindChillC', \n",
        "                'WindGustKmph', 'cloudcover', 'humidity', 'pressure', 'visibility', \n",
        "                'windspeedKmph', 'year', 'month', 'day', 'date_time']\n",
        "\n",
        "        # setting column order\n",
        "        X = X[cols]\n",
        "\n",
        "        new_col_names = ['City', 'Postal', 'City_State', 'MaxTempF', 'FeelsLikeF', \n",
        "                         'MinTempF', 'MaxTempC', 'FeelsLikeC', 'MinTempC', 'Precip_mm', \n",
        "                         'TotalSnow_cm', 'UVindex', 'DewPointC', 'HeatIndexC', 'WindChillC', \n",
        "                         'WindGust_kmph', 'CloudCover', 'Humidity', 'Pressure', 'Visibility', \n",
        "                         'WindSpeed_kmph', 'year', 'month', 'day', 'date_time']\n",
        "\n",
        "        cols_new_cols_dict = dict(zip(cols, new_col_names))\n",
        "\n",
        "        X.rename(columns = cols_new_cols_dict, inplace=True)\n",
        "\n",
        "        return X\n",
        "    ## CALLING FUNCTION: clean\n",
        "    # calling the clean function on the df \n",
        "    # data = clean(weather_data)\n",
        "    data = weather_data\n",
        "    ## PART 2:\n",
        "    # Creating the final dictionary that will get converted to JSON that will hold\n",
        "    # all the relevant weather data\n",
        "    Final_Weather_Data_dict = {}\n",
        "      \n",
        "    for i in data['Postal']:\n",
        "        if i not in Final_Weather_Data_dict:\n",
        "            Final_Weather_Data_dict[i] = {}\n",
        "\n",
        "\n",
        "    ## PART 3:\n",
        "    # Creating a dictionary that holds the City_State as the key and the Postal\n",
        "    # code as the value \n",
        "    def Postal_City_State_Weather_dict_func(df, Final_Weather_dict):\n",
        "        # Grouping the data by City_State, and getting the column means\n",
        "        City_State_groupby = df.groupby('City_State').mean()\n",
        "        # Creating a list of the City_State_groupby_indices\n",
        "        City_State_groupby_indices = City_State_groupby.index.to_list()\n",
        "\n",
        "        # Instantiating a new dictionary to hold the City_States as a key, and the \n",
        "        # Postal codes as values\n",
        "        City_State_Postal_dict = {}\n",
        "        # Looping through the City_States_groupby_indices to create entries in the \n",
        "        # City_State_Postal_dict -- simultaneously adding the postal code snippet,\n",
        "        # (ex.\"NY\") from the final two characters in the City_States string i, to \n",
        "        # the value, at the specified key, in City_State_Postal_dict\n",
        "        for i in City_State_groupby_indices:\n",
        "            City_State_Postal_dict[i] = i[-2:]\n",
        "\n",
        "        # Looping through the City_State_Postal_dict (keys: City_State)\n",
        "        for i in City_State_Postal_dict:\n",
        "            # If the (value: Postal) is in the Final_Weather_dict (keys: Postal)\n",
        "            if City_State_Postal_dict[i] in Final_Weather_dict:\n",
        "                # Set the Final_Weather_Dict (key: Postal) at the specified (value: City_State)\n",
        "                # as the City_State\n",
        "                Final_Weather_dict[City_State_Postal_dict[i]][i] = i\n",
        "\n",
        "        # Return the Final_Weather_dict\n",
        "        return Final_Weather_dict\n",
        "\n",
        "    ## CALLING FUNCTION: Postal_City_State_Weather_dict_func\n",
        "    Final_Weather_Data_dict = Postal_City_State_Weather_dict_func(data, Final_Weather_Data_dict)\n",
        "      \n",
        "    ## PART 4:\n",
        "    # Split data into a summer_df and winter_df for the respective timeframes\n",
        "    # summer_df = April to September\n",
        "    summer_df = data[(data['month'] >= 4) & (data['month'] <= 9)]\n",
        "    summer_df = summer_df.copy()\n",
        "    # winter_df = October to March\n",
        "    winter_df = data[(data['month'] < 4) | (data['month'] > 9)]\n",
        "    winter_df = winter_df.copy()\n",
        "\n",
        "\n",
        "    ## PART 5:\n",
        "    # Creating a city_state_avg_dict that holds all the mean values, for the\n",
        "    # respective columns, in the respective season\n",
        "    summer_city_state_avg = summer_df.groupby('City_State').mean()\n",
        "    summer_city_state_avg_dict = summer_city_state_avg.to_dict(orient='index')\n",
        "\n",
        "    winter_city_state_avg = winter_df.groupby('City_State').mean()\n",
        "    winter_city_state_avg_dict = winter_city_state_avg.to_dict(orient='index')\n",
        "\n",
        "\n",
        "    ## PART 6:\n",
        "    def Postal_City_State_Weather_dict_FINAL_func(df, Final_Weather_dict):\n",
        "        # Instantiate an empty dictionary to hold the City_State as the key and the\n",
        "        # Weather data as the value\n",
        "        City_State_Weather_dict = {}\n",
        "\n",
        "        # Looping through the City_State\n",
        "        for j in df['City_State']:\n",
        "            # If it's NOT IN the City_State_Weather_dict\n",
        "            if j not in City_State_Weather_dict:\n",
        "                # Instantiate an empty dictionary as the value in City_State_Weather_dict \n",
        "                City_State_Weather_dict[j] = {}\n",
        "        \n",
        "        # Looping through the City_State in the City_State_Weather_dict \n",
        "        for x in City_State_Weather_dict:\n",
        "            # If it IS IN the summer_city_state_avg_dict \n",
        "            if x in summer_city_state_avg_dict:\n",
        "                # Create an entry called 'summer' in the City_State_Weather_dict value\n",
        "                # space which holds the weather data from the summer_city_state_avg_dict\n",
        "                # at the specified City_State\n",
        "                City_State_Weather_dict[x]['summer'] = summer_city_state_avg_dict[x]\n",
        "\n",
        "        # Doing the same as above for the winter_city_state_avg_dict \n",
        "        for x in City_State_Weather_dict:\n",
        "            if x in winter_city_state_avg_dict:\n",
        "                City_State_Weather_dict[x]['winter'] = winter_city_state_avg_dict[x]\n",
        "\n",
        "        # Looping through the postal codes in Final_Weather_dict  \n",
        "        for postal in Final_Weather_dict:\n",
        "            # Looping through the city_states in the postal dictionary\n",
        "            for city_state in Final_Weather_dict[postal]:\n",
        "                # If the city_state is in the City_State_Weather_dict \n",
        "                if city_state in City_State_Weather_dict:\n",
        "                    # Set the Final_Weather_dict value for the city_state, in the \n",
        "                    # respective postal code, as the weather data with the respective seasons \n",
        "                    Final_Weather_dict[postal][city_state] = City_State_Weather_dict[city_state]\n",
        "\n",
        "        # Return the Final_Weather_Dict\n",
        "        return Final_Weather_dict\n",
        "    \n",
        "    ## CALLING FUNCTION: Postal_City_State_Weather_dict_FINAL_func\n",
        "    FINAL_Weather_dict = Postal_City_State_Weather_dict_FINAL_func(data, Final_Weather_Data_dict)\n",
        "    \n",
        "    ## JSON  \n",
        "    weather_json = json.dumps(FINAL_Weather_dict)\n",
        "    \n",
        "    # return\n",
        "    return weather_json\n",
        "\n",
        "\n",
        "## CHECK:\n",
        "weather_JSON = weather_json_func(weather_data)\n",
        "weather_JSON"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kLg85Z5ni4u"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sQH-cPxni7b"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJweAPjMni92"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIyftHLDnjAT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGdskb_RnjCu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-yhoXfJnjFE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BozGeM3RnjHk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E37v0MgVnjKb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIYfwx3anjM_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7c7-j4LnjPT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}